# #improved_model_setup.py
# # Script Ä‘á»ƒ táº¡o model files vá»›i kháº¯c phá»¥c overfitting
#
# import json
# import pandas as pd
# import numpy as np
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.model_selection import train_test_split, cross_val_score
# from sklearn.preprocessing import LabelEncoder
# from sklearn.impute import SimpleImputer
# from sklearn.metrics import mean_squared_error, mean_absolute_error
# import joblib
# import os
# import warnings
#
# warnings.filterwarnings('ignore')
#
# print("ğŸ  Improved Model Setup - Kháº¯c phá»¥c overfitting...")
#
# # Táº¡o thÆ° má»¥c models
# os.makedirs("models", exist_ok=True)
#
# try:
#     # Äá»c dá»¯ liá»‡u
#     print("ğŸ“Š Äá»c dá»¯ liá»‡u tá»« batdongsan_data.json...")
#     with open('data/batdongsan_data.json', 'r', encoding='utf-8') as f:
#         data = json.load(f)
#
#     df = pd.DataFrame(data)
#     print(f"Loaded {len(df)} records")
#
#     # 1. Xá»¬ LÃ Dá»® LIá»†U Vá»šI DUPLICATE REMOVAL
#     print("ğŸ” XÃ³a dá»¯ liá»‡u trÃ¹ng láº·p...")
#
#     # XÃ³a duplicate hoÃ n toÃ n
#     initial_count = len(df)
#     df = df.drop_duplicates()
#
#     # XÃ³a duplicate theo cÃ¡c cá»™t quan trá»ng
#     important_cols = ['title', 'price', 'area', 'district', 'province']
#     available_cols = [col for col in important_cols if col in df.columns]
#     df = df.drop_duplicates(subset=available_cols, keep='first')
#
#     # XÃ³a duplicate theo URL náº¿u cÃ³
#     if 'url' in df.columns:
#         df = df.drop_duplicates(subset=['url'], keep='first')
#
#     removed_count = initial_count - len(df)
#     print(f"âœ… ÄÃ£ xÃ³a {removed_count} báº£n ghi trÃ¹ng láº·p ({removed_count / initial_count * 100:.1f}%)")
#     print(f"CÃ²n láº¡i: {len(df)} báº£n ghi unique")
#
#     # 2. FEATURE ENGINEERING NÃ‚NG CAO
#     print("ğŸ”§ Feature Engineering nÃ¢ng cao...")
#
#
#     # Chuyá»ƒn Ä‘á»•i bedrooms vÃ  bathrooms vá» numeric
#     def convert_to_numeric(value):
#         if pd.isna(value) or value is None:
#             return np.nan
#         try:
#             return float(value)
#         except:
#             return np.nan
#
#
#     df['bedrooms'] = df['bedrooms'].apply(convert_to_numeric)
#     df['bathrooms'] = df['bathrooms'].apply(convert_to_numeric)
#
#     # Táº¡o features má»›i
#     df['has_bedroom_info'] = df['bedrooms'].notna().astype(int)
#     df['has_bathroom_info'] = df['bathrooms'].notna().astype(int)
#     df['total_rooms'] = df['bedrooms'].fillna(0) + df['bathrooms'].fillna(0)
#
#     # Features interaction má»›i
#     df['price_per_sqm'] = df['price'] / df['area']  # CÃ³ thá»ƒ dÃ¹ng Ä‘á»ƒ validate
#     df['room_density'] = df['total_rooms'] / df['area']  # Máº­t Ä‘á»™ phÃ²ng
#
#     # Feature encoding cho district phá»• biáº¿n
#     district_counts = df['district'].value_counts()
#     top_districts = district_counts.head(10).index.tolist()
#     df['is_popular_district'] = df['district'].isin(top_districts).astype(int)
#
#     # 3. Xá»¬ LÃ OUTLIERS Tá»Tá»T HÆ N
#     print("ğŸ¯ Xá»­ lÃ½ outliers báº±ng IQR method...")
#
#     # Sá»­ dá»¥ng IQR Ä‘á»ƒ loáº¡i bá» outliers
#     for col in ['price', 'area']:
#         Q1 = df[col].quantile(0.25)
#         Q3 = df[col].quantile(0.75)
#         IQR = Q3 - Q1
#         lower_bound = Q1 - 1.5 * IQR
#         upper_bound = Q3 + 1.5 * IQR
#
#         before_count = len(df)
#         df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
#         after_count = len(df)
#
#         print(f"Removed {before_count - after_count} outliers from {col}")
#
#     print(f"Data sau khi xá»­ lÃ½ outliers: {len(df)} records")
#
#     # 4. CHUáº¨N Bá»Š Dá»® LIá»†U
#     features = ['area', 'bedrooms', 'bathrooms', 'district', 'province',
#                 'has_bedroom_info', 'has_bathroom_info', 'total_rooms',
#                 'room_density', 'is_popular_district']
#
#     df_model = df[features + ['price']].copy()
#     df_model = df_model.dropna(subset=['price'])
#
#     print(f"Data for modeling: {len(df_model)} records")
#
#     # Xá»­ lÃ½ missing values
#     numeric_features = ['area', 'bedrooms', 'bathrooms', 'total_rooms', 'room_density']
#     imputer = SimpleImputer(strategy='median')
#     df_model[numeric_features] = imputer.fit_transform(df_model[numeric_features])
#
#     # Xá»­ lÃ½ categorical features
#     categorical_features = ['district', 'province']
#     for col in categorical_features:
#         df_model[col] = df_model[col].fillna('Unknown')
#
#     # Encode categorical variables
#     label_encoders = {}
#     for col in categorical_features:
#         le = LabelEncoder()
#         df_model[col + '_encoded'] = le.fit_transform(df_model[col])
#         label_encoders[col] = le
#
#     # Prepare final features
#     final_features = [col for col in numeric_features] + \
#                      [col + '_encoded' for col in categorical_features] + \
#                      ['has_bedroom_info', 'has_bathroom_info', 'is_popular_district']
#
#     X = df_model[final_features]
#     y = df_model['price']
#
#     print(f"Features: {final_features}")
#     print(f"X shape: {X.shape}")
#
#     # 5. CHIA Dá»® LIá»†U VÃ€ TRAIN MODEL
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#
#     # Train model vá»›i hyperparameters chá»‘ng overfitting
#     print("ğŸŒ² Training Random Forest vá»›i regularization...")
#     model = RandomForestRegressor(
#         n_estimators=100,
#         max_depth=10,  # Giáº£m tá»« 15 xuá»‘ng 10
#         min_samples_split=10,  # TÄƒng tá»« 5 lÃªn 10
#         min_samples_leaf=5,  # TÄƒng tá»« 2 lÃªn 5
#         max_features='sqrt',  # ThÃªm Ä‘á»ƒ giáº£m overfitting
#         random_state=42,
#         n_jobs=-1
#     )
#
#     model.fit(X_train, y_train)
#
#     # 6. ÄÃNH GIÃ MODEL TOÃ€N DIá»†N
#     print("\nğŸ“Š ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t model...")
#
#     # Predictions
#     y_train_pred = model.predict(X_train)
#     y_test_pred = model.predict(X_test)
#
#     # Scores
#     train_score = model.score(X_train, y_train)
#     test_score = model.score(X_test, y_test)
#
#     # Error metrics
#     train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
#     test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
#     train_mae = mean_absolute_error(y_train, y_train_pred)
#     test_mae = mean_absolute_error(y_test, y_test_pred)
#
#     # Cross validation
#     cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')
#
#     print(f"\nğŸ¯ Káº¾T QUáº¢ ÄÃNH GIÃ:")
#     print(f"Train RÂ² Score: {train_score:.4f}")
#     print(f"Test RÂ² Score: {test_score:.4f}")
#     print(f"Overfitting gap: {abs(train_score - test_score):.4f}")
#     print(f"\nTrain RMSE: {train_rmse:.2f} tá»· VNÄ")
#     print(f"Test RMSE: {test_rmse:.2f} tá»· VNÄ")
#     print(f"Train MAE: {train_mae:.2f} tá»· VNÄ")
#     print(f"Test MAE: {test_mae:.2f} tá»· VNÄ")
#     print(f"\nCV RÂ² Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
#
#     # Feature importance
#     print(f"\nğŸ” TOP 5 FEATURES:")
#     feature_importance = pd.DataFrame({
#         'feature': final_features,
#         'importance': model.feature_importances_
#     }).sort_values('importance', ascending=False)
#
#     for i, row in feature_importance.head().iterrows():
#         print(f"{row['feature']}: {row['importance']:.4f}")
#
#     # 7. LÆ¯U MODEL
#     print("\nğŸ’¾ Saving improved model files...")
#
#     joblib.dump(model, 'models/real_estate_model_improved.pkl')
#     print("âœ… Saved: models/real_estate_model_improved.pkl")
#
#     joblib.dump(label_encoders, 'models/label_encoders_improved.pkl')
#     print("âœ… Saved: models/label_encoders_improved.pkl")
#
#     joblib.dump(imputer, 'models/imputer_improved.pkl')
#     print("âœ… Saved: models/imputer_improved.pkl")
#
#     # LÆ°u feature names
#     with open('models/feature_names.json', 'w') as f:
#         json.dump(final_features, f)
#     print("âœ… Saved: models/feature_names.json")
#
#     print("\nğŸ‰ Improved model setup completed!")
#
#     # 8. KIá»‚M TRA SAMPLE PREDICTION
#     print("\nğŸ§ª Testing improved prediction...")
#     sample_data = pd.DataFrame({
#         'area': [100.0],
#         'bedrooms': [3.0],
#         'bathrooms': [2.0],
#         'total_rooms': [5.0],
#         'room_density': [0.05],
#         'district_encoded': [0],
#         'province_encoded': [0],
#         'has_bedroom_info': [1],
#         'has_bathroom_info': [1],
#         'is_popular_district': [1]
#     })
#
#     predicted_price = model.predict(sample_data[final_features])[0]
#     print(f"Sample prediction (100mÂ², 3BR, 2BA): {predicted_price:.2f} tá»· VNÄ")
#
#     # ÄÃ¡nh giÃ¡ cáº£i thiá»‡n
#     overfitting_gap = abs(train_score - test_score)
#     if overfitting_gap < 0.15:
#         print(f"\nâœ… THÃ€NH CÃ”NG: Overfitting gap giáº£m xuá»‘ng {overfitting_gap:.4f} (<15%)")
#     else:
#         print(f"\nâš ï¸ CHÃš Ã: Overfitting gap váº«n cao {overfitting_gap:.4f} (>15%)")
#
# except FileNotFoundError as e:
#     print(f"âŒ Error: File not found - {e}")
#     print("Please check your data file path.")
# except Exception as e:
#     print(f"âŒ Error: {e}")
#     import traceback
#
#     traceback.print_exc()
#
# print("\nğŸ”š Script completed. Safe to close.")  # ThÃªm dÃ²ng nÃ y Ä‘á»ƒ trÃ¡nh exception cuá»‘i